# 🏠 GAVIS BOT 프로젝트

GAVIS 유래 : 자비스같은 AI 통합체제를 만들고 싶지만, 시작은 간단하게 개비스로 시작

가정 내에서 사용자의 음성/얼굴 인식을 기반으로 대화하며,  
간단한 물건(수건, 양말 등)을 옮길 수 있는 **저비용 홈 어시스턴트 로봇** 프로젝트  

---

## 📌 요구사항
1. **대화 입력**: LLM 모델 기반 명령 처리 (GPT, Gemini 등 무료 모델 선호)  
2. **네비게이션**: 저렴한 센서(LiDAR, 카메라) 기반 SLAM/경로계획  
3. **얼굴 인식**: 사용자 구분 가능  
4. **음성 인식**: 음성 명령 이해  
5. **페이로드**: 모듈화 구조 (시스템 종속 X, 다양한 장착 가능)  
6. **제어 구조**: 상위 제어 ↔ 하위 제어 분리  
7. **네트워크**: 가정용 Wi-Fi 연결  
8. **물리 이동성**: 문턱 극복 가능  
9. **재미 요소**: 가끔 사용자에게 장난 기능  
10. **배터리 관리**: 충전 필요 시 충전 장소로 자동 이동  
11. **디스플레이**: 로봇 얼굴을 표현하는 화면 필요  

---

## 🧩 소프트웨어 아키텍처

### 상위 제어
- LLM API (GPT, Gemini)  
- STT/TTS (Google STT, Whisper, Coqui TTS 등)  
- 얼굴/음성 인식 (OpenCV + FaceNet, Vosk 등)  
- 디스플레이 제어 (표정, UI, 상호작용)  
- Fun Mode (장난 기능)  

### 중간 제어
- SLAM (RTAB-Map, ORB-SLAM2)  
- 경로 계획 (Nav2, Move Base)  
- 미션 매니저 (Task Scheduler)  

### 하위 제어
- 플랫폼 SDK (TurtleBot4 SDK, JetBot SDK)  
- 센서 데이터 처리 (LiDAR, 카메라, IMU)  
- 페이로드 제어 API (그리퍼, 로봇팔 등)  
- 배터리 상태 모니터링  

---

## ⚙️ 하드웨어 구성

### 플랫폼
- **TurtleBot4 Lite** → 안정적, LiDAR 기반 내비게이션, SDK 지원  
- **Nvidia JetBot** → 저렴, 카메라 기반 SLAM, 소형 환경에 적합  

### 센서
- 2D LiDAR (예: RPLiDAR A1/A2) **또는** Depth Camera (Intel RealSense D435)  
- IMU (플랫폼 기본 포함)  

### 페이로드
- 소형 그리퍼  
- myCobot 미니 로봇팔 (선택적)  

### 입출력 장치
- USB 마이크  
- USB/블루투스 스피커  
- 7인치 라즈베리파이 LCD (로봇 얼굴 표현)  

### 네트워크
- 가정용 Wi-Fi (플랫폼 내장 모듈 활용)  

---

## 📊 시스템 구조 다이어그램
```mermaid
flowchart TD
    A[사용자 음성/텍스트 입력] --> B[상위 제어: LLM + STT/TTS + 얼굴인식]
    B --> C[중간 제어: SLAM + 경로계획 + 미션매니저]
    C --> D[하위 제어: SDK + 센서 + 페이로드 + 배터리]
    D --> E[하드웨어: 플랫폼 + 센서 + I/O + 페이로드]
    E -->|결과| F[작업 수행 및 디스플레이 표현]


📌 개발 마일스톤 (총 6개월)
🔹 1단계: 기본 플랫폼 확보 및 환경 세팅 (1개월차)

 로봇 플랫폼 구매/조립 (TurtleBot4 Lite 또는 JetBot)

 개발 환경 세팅 (Ubuntu + ROS2 + Python SDK)

 원격 제어 테스트 (노트북에서 ROS2 토픽 송수신 확인)

 Wi-Fi 연결 및 원격 SSH 접속 확인

🔹 2단계: 자율 주행 및 내비게이션 (2개월차)

 SLAM 맵핑 (LiDAR 또는 카메라 기반 RTAB-Map/ORB-SLAM2)

 경로 계획 및 자율 주행 (Nav2, Move Base 활용)

 문턱 극복 및 장애물 회피 동작 검증

 "충전 장소" 좌표 지정 → 배터리 부족 시 자동 이동 기능 구현

🔹 3단계: AI 대화 및 인식 기능 (3개월차)

 LLM API 연동 (GPT 무료 모델, Gemini API 등)

 STT (Google STT, Whisper 등) + TTS (Coqui TTS 등) 적용

 음성 명령을 제어 명령으로 변환 파이프라인 구축

 얼굴 인식 (OpenCV + FaceNet) → 사용자 식별 기능 구현

🔹 4단계: 디스플레이 & 사용자 인터랙션 (4개월차)

 라즈베리파이 7인치 디스플레이 연결

 기본 UI/얼굴 애니메이션 구현 (눈 깜빡임, 감정 표현)

 대화와 연동된 디스플레이 표정 변화

 사용자 맞춤 반응 (예: 특정 인물 인식 시 "안녕 OOO!")

🔹 5단계: 페이로드 모듈 & 장난 기능 (5개월차)

 페이로드 모듈 장착 (소형 그리퍼 또는 myCobot)

 간단한 물건 집기/운반 기능 테스트

 "Fun Mode" (장난 기능) 개발

예: 갑자기 춤추기, 농담 말하기, 사용자를 따라다니기

🔹 6단계: 통합 & 최종 테스트 (6개월차)

 모든 기능 통합 (네비게이션 + 대화 + 인식 + 페이로드)

 실제 집 환경에서 시나리오 테스트

사용자가 “양말 가져와” → 음성 인식 → 내비게이션 → 운반 → 디스플레이로 반응

 버그 수정 및 안정화

 프로젝트 문서화 및 GitHub 공개
