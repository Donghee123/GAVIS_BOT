📌 개발 마일스톤 (총 6개월)

🔹 1단계: 기본 플랫폼 확보 및 환경 세팅 (1개월차) (~25.10.01)

 로봇 플랫폼 구매/조립 (TurtleBot4 Lite 또는 JetBot)

 개발 환경 세팅 (Ubuntu + ROS2 + Python SDK)

 원격 제어 테스트 (노트북에서 ROS2 토픽 송수신 확인)

 Wi-Fi 연결 및 원격 SSH 접속 확인
 

🔹 2단계: 자율 주행 및 내비게이션 (2개월차) (~25.11.01)

 SLAM 맵핑 (LiDAR 또는 카메라 기반 RTAB-Map/ORB-SLAM2)

 경로 계획 및 자율 주행 (Nav2, Move Base 활용)

 문턱 극복 및 장애물 회피 동작 검증

 "충전 장소" 좌표 지정 → 배터리 부족 시 자동 이동 기능 구현
 

🔹 3단계: AI 대화 및 인식 기능 (3개월차) (~25.12.01)

 LLM API 연동 (GPT 무료 모델, Gemini API 등)

 STT (Google STT, Whisper 등) + TTS (Coqui TTS 등) 적용

 음성 명령을 제어 명령으로 변환 파이프라인 구축

 얼굴 인식 (OpenCV + FaceNet) → 사용자 식별 기능 구현
 

🔹 4단계: 디스플레이 & 사용자 인터랙션 (4개월차) (~26.01.01)

 라즈베리파이 7인치 디스플레이 연결

 기본 UI/얼굴 애니메이션 구현 (눈 깜빡임, 감정 표현)

 대화와 연동된 디스플레이 표정 변화

 사용자 맞춤 반응 (예: 특정 인물 인식 시 "안녕 OOO!")
 

🔹 5단계: 페이로드 모듈 & 장난 기능 (5개월차) (~26.02.01)

 페이로드 모듈 장착 (소형 그리퍼 또는 myCobot)

 간단한 물건 집기/운반 기능 테스트

 "Fun Mode" (장난 기능) 개발

예: 갑자기 춤추기, 농담 말하기, 사용자를 따라다니기


🔹 6단계: 통합 & 최종 테스트 (6개월차) (~26.03.01)

 모든 기능 통합 (네비게이션 + 대화 + 인식 + 페이로드)

 실제 집 환경에서 시나리오 테스트

사용자가 “양말 가져와” → 음성 인식 → 내비게이션 → 운반 → 디스플레이로 반응

 버그 수정 및 안정화

 프로젝트 문서화 및 GitHub 공개
